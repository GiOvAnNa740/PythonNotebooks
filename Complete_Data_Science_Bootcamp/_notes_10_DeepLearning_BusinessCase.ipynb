{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to determine if a person will purchase audiobooks again, based on the inputs.\n",
    "\n",
    "Since this is a supervized learning example, the last column contains our targets, that indicate if the persor returned or not in the following 6 months (1=returned, 0= did not return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On the table corresponding to the evaluation (from 0 to 10) the value was only filled for custumers who left any evaluation.\n",
    "So solve the issue of empty cells, the ones that had no evaluation were filled with the average of the total evaluations = 8.91\n",
    "This will remove empty cell and give the algorithm a basis parameter as comparison for the evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>873</th>\n",
       "      <th>2160</th>\n",
       "      <th>2160.1</th>\n",
       "      <th>10.13</th>\n",
       "      <th>10.13.1</th>\n",
       "      <th>0</th>\n",
       "      <th>8.91</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>2808</td>\n",
       "      <td>6.66</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>705</td>\n",
       "      <td>324.0</td>\n",
       "      <td>324</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>391</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>15.31</td>\n",
       "      <td>15.31</td>\n",
       "      <td>0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>819</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1296</td>\n",
       "      <td>7.11</td>\n",
       "      <td>21.33</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14078</th>\n",
       "      <td>27398</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>7.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14079</th>\n",
       "      <td>28220</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14080</th>\n",
       "      <td>28671</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14081</th>\n",
       "      <td>31134</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>6.14</td>\n",
       "      <td>6.14</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14082</th>\n",
       "      <td>32832</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14083 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         873    2160  2160.1  10.13  10.13.1  0  8.91   0.1  0.2  0.3  0.4  1\n",
       "0        611  1404.0    2808   6.66    13.33  1  6.50  0.00  0.0    0  182  1\n",
       "1        705   324.0     324  10.13    10.13  1  9.00  0.00  0.0    1  334  1\n",
       "2        391  1620.0    1620  15.31    15.31  0  9.00  0.00  0.0    0  183  1\n",
       "3        819   432.0    1296   7.11    21.33  1  9.00  0.00  0.0    0    0  1\n",
       "4        138  2160.0    2160  10.13    10.13  1  9.00  0.00  0.0    0    5  1\n",
       "...      ...     ...     ...    ...      ... ..   ...   ...  ...  ...  ... ..\n",
       "14078  27398  2160.0    2160   7.99     7.99  0  8.91  0.00  0.0    0   54  0\n",
       "14079  28220  1620.0    1620   5.33     5.33  1  9.00  0.61  0.0    0    4  0\n",
       "14080  28671  1080.0    1080   6.55     6.55  1  6.00  0.29  0.0    0   29  0\n",
       "14081  31134  2160.0    2160   6.14     6.14  0  8.91  0.00  0.0    0    0  0\n",
       "14082  32832  1620.0    1620   5.33     5.33  1  8.00  0.38  0.0    0   90  0\n",
       "\n",
       "[14083 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('db/Audiobooks_data.csv')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data as an array\n",
    "\n",
    "raw_data = np.loadtxt('db/Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "unscaled_inputs_all = raw_data[:,1:-1] # all lines, all columns except for the IDs (first column)\n",
    "\n",
    "targets_all = raw_data[:,-1] #all lines, only the last column 'target'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the data\n",
    "\n",
    "It is important to have a good variety of data, or the accuracy may seem good, but the model is not really making good predictions, you just happen to have a lot more of one type of the data\n",
    "\n",
    "well distributed data will better show the accuracy of the model, mainly when it is a classification algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    On this dataset, we have a lot more custumers that did not return, so we will balance the inputs before proceeding, so there will be as many 0's as there are 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all)) #sometimes the number can come as a boolean, so we're declaring it as an integer, this returns the total number of '1' targets\n",
    "zero_target_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "#we will count the number of 0's and compare to the unmber of 1's\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] ==0:\n",
    "        zero_target_counter += 1\n",
    "        if zero_target_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# remove exceeding 0's from the data\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0) #.delete(dataset, data to remove)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the indices and then getting the related data\n",
    "\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792.0 3579 0.5006985191394244\n",
      "226.0 447 0.5055928411633109\n",
      "219.0 448 0.4888392857142857\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count) # 80% of the data\n",
    "validation_samples_count = int(0.1*samples_count) # 10% of the data\n",
    "test_samples_count = samples_count-train_samples_count-validation_samples_count # ~10% of the data\n",
    "\n",
    "#first 80%\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "#next 10%\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "#next 10%\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "#printing the quantities\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataset\n",
    "\n",
    "storing the shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)\n",
    "\n",
    "#keyword 'input' and 'targets' are only variables, you may use any keywords you preffer, these are being used for convenience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Outlining\n",
    "\n",
    "10 inputs on the input layer\n",
    "\n",
    "2 hidden layers with 50 units each\n",
    "\n",
    "1 outputs on the output layer (1 = custumer returned, 0 = customer did not return)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "train_targets = npz['targets'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "validation_inputs = npz['inputs'].astype(float)\n",
    "validation_targets = npz['targets'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs = npz['inputs'].astype(float)\n",
    "test_targets = npz['targets'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Preprocessing usaully takes the most effort as it needs to be specifically designed for each dataset\n",
    "\n",
    "The modeling itself usually can be applyed with little changes through a variety of datasests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10 #10 columns for inputs categories\n",
    "output_size = 2 #two possible output values (0 or 1)\n",
    "hidden_layer = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #input layer does not need to be declared since we already preprocessed the data prior to this point\n",
    "    #hidden layers\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'), #'relu' = activaion function\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'),\n",
    "    #output layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') #'sofmax' = activation function -> transforms the values into probabilities\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss funtion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions:\n",
    "\n",
    "-   binary_crossentropy -> used when we have binary data encoding\n",
    "\n",
    "-   categorical_crossentropy -> expects that the data is already one-hot encoded\n",
    "\n",
    "-   sparse_categorical_crossentropy -> applies one-hot encoding to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer, loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', # ADAM = adaptive moment estimation\n",
    "              metrics=['accuracy']) # include metrics that we wish to be calculated during the training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.5937 - accuracy: 0.7368 - val_loss: 0.4565 - val_accuracy: 0.8613 - 652ms/epoch - 18ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3833 - accuracy: 0.8754 - val_loss: 0.3443 - val_accuracy: 0.8680 - 53ms/epoch - 1ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3201 - accuracy: 0.8826 - val_loss: 0.3226 - val_accuracy: 0.8770 - 52ms/epoch - 1ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.2996 - accuracy: 0.8919 - val_loss: 0.3052 - val_accuracy: 0.8792 - 53ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.2853 - accuracy: 0.8941 - val_loss: 0.2936 - val_accuracy: 0.8814 - 60ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.2750 - accuracy: 0.8986 - val_loss: 0.2862 - val_accuracy: 0.8904 - 64ms/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.2681 - accuracy: 0.9014 - val_loss: 0.2846 - val_accuracy: 0.8949 - 58ms/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.2619 - accuracy: 0.9014 - val_loss: 0.2868 - val_accuracy: 0.8904 - 53ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.2580 - accuracy: 0.9033 - val_loss: 0.2750 - val_accuracy: 0.8949 - 55ms/epoch - 2ms/step\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.2541 - accuracy: 0.9022 - val_loss: 0.2748 - val_accuracy: 0.8949 - 55ms/epoch - 2ms/step\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.2497 - accuracy: 0.9061 - val_loss: 0.2697 - val_accuracy: 0.8971 - 54ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.2478 - accuracy: 0.9070 - val_loss: 0.2658 - val_accuracy: 0.8993 - 51ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.2461 - accuracy: 0.9075 - val_loss: 0.2731 - val_accuracy: 0.8993 - 55ms/epoch - 2ms/step\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.2437 - accuracy: 0.9095 - val_loss: 0.2633 - val_accuracy: 0.8971 - 55ms/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.2408 - accuracy: 0.9089 - val_loss: 0.2637 - val_accuracy: 0.8993 - 53ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "36/36 - 0s - loss: 0.2387 - accuracy: 0.9111 - val_loss: 0.2711 - val_accuracy: 0.8971 - 53ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17718400250>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "# this is a stopping mechanism available on the Keras library (check documentation for more options)\n",
    "# it will always compare the validation loss with the previous vallue, if it starts increasing it will interrupt the epochs to prevent overfitting\n",
    "# this way, we do not need to know the precise number of epochs to not overfit the model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2) #the patience will allow the model to tolerate some increases on valisation loss before stopping\n",
    "\n",
    "#too many epochs can mean model overfitting\n",
    "\n",
    "model.fit(train_inputs,train_targets,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs,validation_targets),\n",
    "          verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Validation Accuracy = 89.71% (5.8 sec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 0s - loss: 0.2403 - accuracy: 0.9085 - 28ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24031133949756622, 0.9084821343421936]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_inputs,test_targets,\n",
    "               verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Accuracy: 90.85%\n",
    "\n",
    "It is usually smaler than or equal to the validation accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
