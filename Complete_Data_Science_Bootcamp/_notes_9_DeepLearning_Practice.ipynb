{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Deep Learning - The MNIST Dataset\n",
    "\n",
    "\"Hello World\" of Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in Grayscale from 0 to 255, in which 0 corresponds to pure black, and 255 to pure white\n",
    "\n",
    "Each picture has 784 pixels, so our input layer will be an array of 784 inputs\n",
    "\n",
    "We will create 2 hidden layers with the same width, and an output layer with 10 units (since there are 10 digits to identify)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "-   1 - Prepare and preprocess the data. Split the Training, Validation and Test datasets.\n",
    "\n",
    "-   2 - Outline the model and choose the Activation Functions\n",
    "\n",
    "-   3 - Set the approapriate advamced optimizers and loss function\n",
    "\n",
    "-   4 - Train it and make it learn, validating in each epoch\n",
    "\n",
    "-   5 - Test the accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True) #loading the dataset from 'tensorflow_datasets'\n",
    "# 'as_supervised=True' will load the data in 2 tuples: input and target\n",
    "\n",
    "#Dataset stored at C:\\Users\\Pichau\\tensorflow_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting tha data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] # by default tensors only split into train and test\n",
    "\n",
    "#splitting the validation samples from the train data\n",
    "\n",
    "num_val_samples = 0.1 * mnist_info.splits['train'].num_examples # this will return the total number of training samples divided by 10\n",
    "num_val_samples = tf.cast(num_val_samples,tf.int64) # since the previous result may not be an integer, here we overwrite it with 'tf.cast', to convert it to an integer\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples # this will return the total number of test samples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64) # this is just to guarantee the output will be an integer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ! tf.cast converts the values to a set data type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scale the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scale the inputs\n",
    "\n",
    "def scale(image,label):\n",
    "  image= tf.cast(image,tf.float32) #ensuring the image input will be a float\n",
    "  #255 is the total possible values each pixel can receive\n",
    "  image/=255. # this will scale the inputs to a range 0 -> 1. The dot at the end states once more that the value should be a float\n",
    "  return image, label\n",
    "\n",
    "scaled_train_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data\n",
    "\n",
    "Very important to prevent that the train set see only some of the possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the scaled data in case the data is ordered (which could compromise the train efficiency)\n",
    "\n",
    "BUFFER_SIZE = 10000 # defines how much data will be taken at each shuffling batch, since the dataset is to big to shuffle it all at once\n",
    "\n",
    "shuffled_train_validation_data = scaled_train_validation_data.shuffle(BUFFER_SIZE) # shuffle method which receives only the buffer size as argument\n",
    "\n",
    "validation_data = shuffled_train_validation_data.take(num_val_samples) # assigning for validation 10% of the shuffled train data\n",
    "\n",
    "train_data = shuffled_train_validation_data.skip(num_val_samples) # will get all data except for the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will add a new column to the tensor which indicates how many samples it should take on each batch\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE) # combines consecutive elements into batches\n",
    "\n",
    "#for validation and test we dont need to separate the batches, but still need to put the whole set into a batch\n",
    "validation_data = validation_data.batch(num_val_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) # 'next' loads the next element of an iterable object, and 'iter' makes the data iterable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions, optimizers, loss functions, etc are all selected from the TF library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to create dedicated variables for some of the parameters so that we can easily modify them during when optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #input layer\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), #the input shape corresponds to the size of the image, flatten to a vector\n",
    "    #hidden layers\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'), #'relu' = activaion function\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'),\n",
    "    #output layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') #'sofmax' = activaion function -> this funtion will transform the values into probabilities\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss funtion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions:\n",
    "\n",
    "-   binary_crossentropy -> used when we have binary data encoding\n",
    "\n",
    "-   categorical_crossentropy -> expects that the data is already one-hot encoded\n",
    "\n",
    "-   sparse_categorical_crossentropy -> applies one-hot encoding to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer, loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', # ADAM = adaptive moment estimation\n",
    "              metrics=['accuracy']) # include metrics that we wish to be calculated during the training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 2s - loss: 0.0517 - accuracy: 0.9839 - val_loss: 0.0647 - val_accuracy: 0.9808 - 2s/epoch - 3ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.0484 - val_accuracy: 0.9867 - 2s/epoch - 3ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0363 - accuracy: 0.9886 - val_loss: 0.0467 - val_accuracy: 0.9875 - 2s/epoch - 3ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.0302 - accuracy: 0.9912 - val_loss: 0.0397 - val_accuracy: 0.9878 - 2s/epoch - 3ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0254 - accuracy: 0.9922 - val_loss: 0.0382 - val_accuracy: 0.9882 - 2s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f050f86580>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "#the model will run the training data in batches for the set number of epochs \n",
    "#then it will run the validation data all at once since there is only one batch\n",
    "model.fit(train_data, epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Analisys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis with original code:\n",
    "\n",
    "Validation accuracy: 97.13% (8.6 sec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on hidden layer size\n",
    "\n",
    "    Hidden layer size changed from 50 to 100\n",
    "\n",
    "Validation accuracy: 98.82% (8.1 sec)\n",
    "\n",
    "    Hidden layer size changed from 100 to 200\n",
    "\n",
    "Validation accuracy: 98.53% (11.3 sec) -> not a significant improvment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on number of hidden layers\n",
    "\n",
    "    Hidden layers changed from 2 to 3\n",
    "\n",
    "Validation accuracy: 98.17% (10.3 sec) -> no improvment\n",
    "\n",
    "    Hidden layers changed from 2 to 5\n",
    "\n",
    "Validation accuracy: 98.08% (11.5 sec) -> no improvment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on activation function\n",
    "\n",
    "    Changed from 'relu' to 'sigmoid'\n",
    "\n",
    "Validation accuracy: 96.12% (9.9 sec) -> model regressed\n",
    "\n",
    "    Changed from 'relu' to 'tanh' only on the second hidden layer\n",
    "\n",
    "Validation accuracy: 98.02% (9.6sec) -> no improvment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on batch size\n",
    "\n",
    "    Changed from 100 to 1000\n",
    "\n",
    "Validation accuracy: 95.80% (6.5 sec) -> model regressed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "Here we will be able to see if the model was overfitted. After this point, we cannot keep changing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0859 - accuracy: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08588937669992447, 0.9754999876022339]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, verbose=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Accuracy: 97.55%\n",
    "\n",
    "A little smaler then the validation accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
