{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Deep Learning - The MNIST Dataset\n",
    "\n",
    "\"Hello World\" of Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in Grayscale from 0 to 255, in which 0 corresponds to pure black, and 255 to pure white\n",
    "\n",
    "Each picture has 784 pixels, so our input layer will be an array of 784 inputs\n",
    "\n",
    "We will create 2 hidden layers with the same width, and an output layer with 10 units (since there are 10 digits to identify)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "-   1 - Prepare and preprocess the data. Split the Training, Validation and Test datasets.\n",
    "\n",
    "-   2 - Outline the model and choose the Activation Functions\n",
    "\n",
    "-   3 - Set the approapriate advamced optimizers and loss function\n",
    "\n",
    "-   4 - Train it and make it learn, validating in each epoch\n",
    "\n",
    "-   5 - Test the accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pichau\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True) #loading the dataset from 'tensorflow_datasets'\n",
    "# 'as_supervised=True' will load the data in 2 tuples: input and target\n",
    "\n",
    "#Dataset stored at C:\\Users\\Pichau\\tensorflow_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting tha data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] # by default tensors only split into train and test\n",
    "\n",
    "#splitting the validation samples from the train data\n",
    "\n",
    "num_val_samples = 0.1 * mnist_info.splits['train'].num_examples # this will return the total number of training samples divided by 10\n",
    "num_val_samples = tf.cast(num_val_samples,tf.int64) # since the previous result may not be an integer, here we overwrite it with 'tf.cast', to convert it to an integer\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples # this will return the total number of test samples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64) # this is just to guarantee the output will be an integer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ! tf.cast converts the values to a set data type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scale the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scale the inputs\n",
    "\n",
    "def scale(image,label):\n",
    "  image= tf.cast(image,tf.float32) #ensuring the image input will be a float\n",
    "  #255 is the total possible values each pixel can receive\n",
    "  image/=255. # this will scale the inputs to a range 0 -> 1. The dot at the end states once more that the value should be a float\n",
    "  return image, label\n",
    "\n",
    "scaled_train_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data\n",
    "\n",
    "Very important to prevent that the train set see only some of the possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the scaled data in case the data is ordered (which could compromise the train efficiency)\n",
    "\n",
    "BUFFER_SIZE = 10000 # defines how much data will be taken at each shuffling batch, since the dataset is to big to shuffle it all at once\n",
    "\n",
    "shuffled_train_validation_data = scaled_train_validation_data.shuffle(BUFFER_SIZE) # shuffle method which receives only the buffer size as argument\n",
    "\n",
    "validation_data = shuffled_train_validation_data.take(num_val_samples) # assigning for validation 10% of the shuffled train data\n",
    "\n",
    "train_data = shuffled_train_validation_data.skip(num_val_samples) # will get all data except for the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will add a new column to the tensor which indicates how many samples it should take on each batch\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE) # combines consecutive elements into batches\n",
    "\n",
    "#for validation and test we dont need to separate the batches, but still need to put the whole set into a batch\n",
    "validation_data = validation_data.batch(num_val_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) # 'next' loads the next element of an iterable object, and 'iter' makes the data iterable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions, optimizers, loss functions, etc are all selected from the TF library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to create dedicated variables for some of the parameters so that we can easily modify them during when optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #input layer\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), #the input shape corresponds to the size of the image, flatten to a vector\n",
    "    #hidden layers\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'), #'relu' = activation function\n",
    "    tf.keras.layers.Dense(hidden_layer, activation='relu'),\n",
    "    #output layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') #'sofmax' = activaion function -> this funtion will transform the values into probabilities\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss funtion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions:\n",
    "\n",
    "-   binary_crossentropy -> used when we have binary data encoding\n",
    "\n",
    "-   categorical_crossentropy -> expects that the data is already one-hot encoded\n",
    "\n",
    "-   sparse_categorical_crossentropy -> applies one-hot encoding to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer, loss)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', # ADAM = adaptive moment estimation\n",
    "              metrics=['accuracy']) # include metrics that we wish to be calculated during the training and testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 - 2s - loss: 0.3369 - accuracy: 0.9034 - val_loss: 0.1664 - val_accuracy: 0.9518 - 2s/epoch - 4ms/step\n",
      "Epoch 2/100\n",
      "540/540 - 1s - loss: 0.1399 - accuracy: 0.9589 - val_loss: 0.1217 - val_accuracy: 0.9623 - 1s/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "540/540 - 1s - loss: 0.1013 - accuracy: 0.9693 - val_loss: 0.0882 - val_accuracy: 0.9742 - 1s/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "540/540 - 1s - loss: 0.0792 - accuracy: 0.9761 - val_loss: 0.0749 - val_accuracy: 0.9770 - 1s/epoch - 2ms/step\n",
      "Epoch 5/100\n",
      "540/540 - 1s - loss: 0.0621 - accuracy: 0.9811 - val_loss: 0.0672 - val_accuracy: 0.9785 - 1s/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "540/540 - 1s - loss: 0.0504 - accuracy: 0.9843 - val_loss: 0.0573 - val_accuracy: 0.9818 - 1s/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "540/540 - 1s - loss: 0.0424 - accuracy: 0.9866 - val_loss: 0.0420 - val_accuracy: 0.9872 - 1s/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "540/540 - 1s - loss: 0.0329 - accuracy: 0.9897 - val_loss: 0.0379 - val_accuracy: 0.9880 - 1s/epoch - 2ms/step\n",
      "Epoch 9/100\n",
      "540/540 - 1s - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0314 - val_accuracy: 0.9907 - 1s/epoch - 2ms/step\n",
      "Epoch 10/100\n",
      "540/540 - 1s - loss: 0.0244 - accuracy: 0.9922 - val_loss: 0.0310 - val_accuracy: 0.9905 - 1s/epoch - 2ms/step\n",
      "Epoch 11/100\n",
      "540/540 - 1s - loss: 0.0225 - accuracy: 0.9925 - val_loss: 0.0284 - val_accuracy: 0.9902 - 1s/epoch - 2ms/step\n",
      "Epoch 12/100\n",
      "540/540 - 1s - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0286 - val_accuracy: 0.9912 - 1s/epoch - 2ms/step\n",
      "Epoch 13/100\n",
      "540/540 - 1s - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.0209 - val_accuracy: 0.9933 - 1s/epoch - 2ms/step\n",
      "Epoch 14/100\n",
      "540/540 - 1s - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.0178 - val_accuracy: 0.9948 - 1s/epoch - 2ms/step\n",
      "Epoch 15/100\n",
      "540/540 - 1s - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.0193 - val_accuracy: 0.9945 - 1s/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "540/540 - 1s - loss: 0.0112 - accuracy: 0.9963 - val_loss: 0.0125 - val_accuracy: 0.9960 - 1s/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "540/540 - 1s - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.0158 - val_accuracy: 0.9948 - 1s/epoch - 2ms/step\n",
      "Epoch 18/100\n",
      "540/540 - 1s - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0209 - val_accuracy: 0.9940 - 1s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22183592220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "#the model will run the training data in batches for the set number of epochs \n",
    "#then it will run the validation data all at once since there is only one batch\n",
    "model.fit(train_data, \n",
    "          epochs=NUM_EPOCHS,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs,validation_targets),\n",
    "          verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting an early stopping mechanism validation accuracy reached 99.40% before the model started to overfit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Analisys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis with original code:\n",
    "\n",
    "Validation accuracy: 97.13% (8.6 sec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on hidden layer size\n",
    "\n",
    "    Hidden layer size changed from 50 to 100\n",
    "\n",
    "Validation accuracy: 98.82% (8.1 sec)\n",
    "\n",
    "    Hidden layer size changed from 100 to 200\n",
    "\n",
    "Validation accuracy: 98.53% (11.3 sec) -> not a significant improvment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on number of hidden layers\n",
    "\n",
    "    Hidden layers changed from 2 to 3\n",
    "\n",
    "Validation accuracy: 98.17% (10.3 sec) -> no improvment\n",
    "\n",
    "    Hidden layers changed from 2 to 5\n",
    "\n",
    "Validation accuracy: 98.08% (11.5 sec) -> no improvment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on activation function\n",
    "\n",
    "    Changed from 'relu' to 'sigmoid'\n",
    "\n",
    "Validation accuracy: 96.12% (9.9 sec) -> model regressed\n",
    "\n",
    "    Changed from 'relu' to 'tanh' only on the second hidden layer\n",
    "\n",
    "Validation accuracy: 98.02% (9.6sec) -> no improvment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change on batch size\n",
    "\n",
    "    Changed from 100 to 1000\n",
    "\n",
    "Validation accuracy: 95.80% (6.5 sec) -> model regressed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "Here we will be able to see if the model was overfitted. After this point, we cannot keep changing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 243ms/step - loss: 0.1153 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.115258127450943, 0.9736999869346619]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, verbose=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Accuracy: 97.37%\n",
    "\n",
    "Smaler then the validation accuracy, which is expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
