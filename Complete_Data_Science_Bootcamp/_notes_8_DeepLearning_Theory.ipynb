{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Neural Networks (Deep Net) - working with Layers\n",
    "\n",
    "Layers are the building blocks of Neural Networks (NN)\n",
    "\n",
    "We have the Input Layer, the Outputs Layer, and in the middle of them, the Hidden Layers\n",
    "\n",
    "Width an Depth are called *hyperparameters*\n",
    "\n",
    "    Width: Number of untis on a Layer\n",
    "\n",
    "Usually the width of the Hidder Layers match the one of the Input Layer\n",
    "\n",
    "    Depth: Number of Hidden Layers\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Activation/Transformation Functions\n",
    "\n",
    "-   Sigmoid (Logistic) - Range (0,1)\n",
    "\n",
    "-   TanH (Hyperbolic Tangent) - Range (-1,1)\n",
    "\n",
    "-   ReLu (Rectified Linear Unit) - Range (0,$\\infty$)\n",
    "\n",
    "-   Softmax - Range (0,1) - Transforms the inputs in valid probability distributions (commonly used on image recognition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Perform derivatives on the outputs to spot the errors and adjust the weights\n",
    "\n",
    "This is the core of the optimization process for Neural Netwoks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "When the model has focused so much on the training set that it \"misses the point\", and drifts away from the logic of the data. So it will not work well for values other than the ones on the training set, and it will capture all the random noises. So it explaind the training data nearly perfectly, but it's not efficient in predicting outputs\n",
    "\n",
    "\n",
    "## Underfitting\n",
    "\n",
    "The model has not understood the underlying logic of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In order to prevent and detect those, we should split our dataset into 3: (this woul not be viable to small data sets)\n",
    "\n",
    "The most common split ratios are 80,10,10 or 70,20,10, most part of the data shoulkd always go to the training set\n",
    "\n",
    "-   1 - Training: Train the model\n",
    "-   2 - Validation: Calculate loss function through forward propagation, without adjusting the weights\n",
    "\n",
    "Usually these 2 first steps are performed several times (with adjustments to the model in between), if the validation loss starts to increase, it means the model is being overfitted, this is when the model needs to stop being trained\n",
    "\n",
    "-   3 - Test: test your model to obtain accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-FOLD Cross Validation\n",
    "\n",
    "Only for when the dataset is not large enought to be divided in 3, the train and validation data are combined into one, and only the test data is splitted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "    Technique for stopping the training before overfitting the model.\n",
    "\n",
    "Some of the options are:\n",
    "\n",
    "-   Using a Preset number of epochs (not recommended since there is no guarantee to guess the right number of epochs and we can end up with an underfitted model)\n",
    "\n",
    "-   Stop when the loss function updates become too small (recall the Gradient decent): We will stop as it approaches zero, before the curve starts rising again ( Usually when it comes close to $0.001$ )\n",
    "\n",
    "-   Validation set strategy: The moment to stop the training is defined by the moment the training and validation loss graph line overlap\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "    Process through which we set the innitial values of the weights. Possible Initializers are:\n",
    "\n",
    "-   Uniform Initializer (np.random.uniform): Chooses the values randomly bur in a uniform manner, each of them has exactly the same probability of being chosen\n",
    "\n",
    "-   Normal initializer: The possibles numbers are picked from a normal mean distribution, so number approaxing zero will have higher probabilitty of being selected\n",
    "\n",
    "The issue with both is that if we end up only with input values too small or too big, we will not be able to represent the whole sigmoid function, so a better approach is to guarantee there will be a wide range of weights, and therefore inputs. To adress these issues, the below initializer was developed\n",
    "\n",
    "-   Xavier/Glorot Initialization (glorot_uniform_initializer): There are the Uniform and the Normal Xavier Initializers\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "    Methods to update the weights in order to decrease the loss and increase accuracy\n",
    "\n",
    "-   GD - Gradient Descent: Update the weights once per epoch. Its velocity and precision depends on the learning rate value: The less efficient method in terms of $time vs result$\n",
    "\n",
    "-   SGD - Stochastistic Gradient: Updates the weights after every batch in real time in a single epoch. The number of updates will depend on the batch size. same logic as the GD but way faster, although a little less accurate\n",
    "\n",
    "The gradient descent may not always be a smooth parable, it may have more than one valley and crests. Our goal is always to reach the Global Valley, but sometimes we may incorrectly fall into a local valley instead. To solve this we can include *Momentum* to the optimization, which should never be ignored\n",
    "\n",
    "It consists on applying the same weight function but adjusted, which is done by multiplying it for $\\alpha$. Conventionally it is used $\\alpha = 0.9$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Learning Rate\n",
    "\n",
    "Helps to define a Learning Rate that is not too small nor too big\n",
    "\n",
    "-   1 - Start from a high learning rate leading to faster training\n",
    "\n",
    "-   2 - Gradually lower the rate as the training goes\n",
    "\n",
    "-   3 - Close to the end of the training, we pick a very small rate to get accurate results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Learning Rate\n",
    "\n",
    "\n",
    "-   1 - Start from a high learning rate leading to faster training\n",
    "\n",
    "-   2 - Exponentialy decrease the learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptative Learning Rates\n",
    "\n",
    "-   AdaGrad (Adaptative Gradient Algorithm)\n",
    "\n",
    "-   RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "-   Adam (Adaptive Moment Estimation) -> based on the previous two, but better. It introduces momentum into the equation for calculating the weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "    Data Transformation\n",
    "\n",
    "Preprocessing is necessary to solve issues related to:\n",
    "\n",
    "-   Compatibility\n",
    "\n",
    "-   Orders of magnitude (data with a larger range with have a bigger impact)\n",
    "\n",
    "-   Generalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Preprocessing\n",
    "\n",
    "-   Relative Metrics\n",
    "\n",
    "-   Logarithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization / Feature Scaling\n",
    "\n",
    "Transform all data in to a standard scale\n",
    "\n",
    "A common way to do it is to subtract the mean from the data and divide it by the standad deviation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Converting each sample to a unit lenght vector using the L2-Norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Dimension Reduction Technique used to combine several variables into a bigger variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data\n",
    "\n",
    "For this type of data we need to make it numerical so the algorithm will be able to understand it.\n",
    "\n",
    "    However, we cannot use ordered numerical values (1,2,3,4..) as it would create an hierarchy on the data where there is none\n",
    "\n",
    "-   Binary Encoding: we turn the numerical 1,2,3 order into a binary code 01,10,11 and so on, but each digit will be on a different collumn variable\n",
    "\n",
    "-   One-Hot encoding: We create dummy variables, including as much new columns as there are possible values, and turn the values into 1's ('yes') and 0's ('no'). Each column will only have one '1' value for when the data belongs to that category ('yes').\n",
    "\n",
    "The problem with the last is that when that are too many categories, a lot of columns would be created, which is not viable, so the binary encoding would be preferable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
